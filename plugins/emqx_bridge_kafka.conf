##====================================================================
## Configuration for EMQX Kafka Bridge
##====================================================================

## Cluster support
## bridge.kafka.servers = 127.0.0.1:9092,127.0.0.2:9092,127.0.0.3:9092
bridge.kafka.servers = 127.0.0.1:9092

## When connecting to kafka 0.9 query_api_versions config entry should be set to false
## otherwise the socket will be closed by kafka.
bridge.kafka.query_api_versions = true

## default per_partition, can also be per_broker.
## This is to configure how client manages connections:
## one connection per-partition or one connection per-broker.
## per_partition may give better throughput, but it could be
## quite exhausting for both beam and kafka cluster when
## there is a great number of partitions
bridge.kafka.connection_strategy = per_partition

## This is to avoid excessive metadata refresh and partition
## leader reconnect when a lot of connections restart around the same moment.
## Also, when kafka partition leader broker is down,
## it usually takes a few seconds to get a new leader elacted,
## hence it is a good idea to have a delay before trying to reconnect.
bridge.kafka.min_metadata_refresh_interval = 5S

## Either sync or async. In sync mode, the MQTT client actor block-waits for
## acknowledgement from Kafka before it handles more messages.
##
## The advantage of using 'sync' produce is that it has more push-backs
## towards fast clients.
## The disadvantage is it may cause the actor to be irresponsive to the
## management actions such as 'kick', and even MQTT heartbeats.
##
## Using 'async' however, means there is no push-back towards MQTT clients.
## In case Kafka is temporarily down, all messages will have to be buffered.
## (The buffer is configurable, please see replayq configs below).
bridge.kafka.produce = sync

## This config only takes effect for 'sync' mode.
## It is the time for MQTT client actor to wait for Kafka's ack.
## A timeout will result in an error level log message, but does not
## cause client to disconnect.
bridge.kafka.produce.sync_timeout = 3s

## Kafka producers can be configured to buffer pending messages on disk.
## The buffer is segmented (size is configurable, see replayq_seg_bytes config)
##
## This value defines the base directory where replayq stores messages on disk.
## If this config entry if missing or set to an empty value,
## replayq works in a mem-only manner.
## i.e. messages are not queued on disk -- in such case,
## the send or send_sync API callers are responsible for
## possible message loss in case of application,
## network or kafka disturbances.
##
## If this directory is configured, pending messages will be buffered on disk.

# bridge.kafka.replayq_dir = data/replayq/

## Kafka producers can be configured to drop old messages from the in-mem buffer of pending
## messages when the system is under high memory pressure.
##
## If set to true and mem-only buffer is enabled, when system memory usage is over the threshold,
## Kafka producer pushes the new data into the buffered queue while popping and dropping the same
## amount of old data from the buffer.
##
## The buffer queue is defined by 'bridge.kafka.replayq_dir'.
## The memory usage threshold is defined by 'os_mon.sysmem_high_watermark'.

# bridge.kafka.highmem_drop = false

## Each Kafka partition may buffer up to this number of bytes.
## When exceeded, the oldest messages will be deleted.
# bridge.kafka.replayq_max_total_bytes = 2GB

## default=false, use replayq in offload mode
## In 'offload' mode the buffer's frontmost segment which is being read off
## and sent to Kafka, is always in RAM. The producer only starts writing
## segments on disk after the front-segment is full (more than replayq_seg_bytes)

# bridge.kafka.replayq_offload_mode = false

## default=10MB, replayq segment size.
# bridge.kafka.producer.replayq_seg_bytes = 10MB

## Configre how the producer should receive acknowledgement from Kafka
## possible values: 'all_isr', 'leader_only' or 'none'.
## 'all_isr': Kafka should ack only when all the in-sync replicas have handled
##            the produce request.
## 'leader_only': Kafka can ack right after the partition leader has handled
##                the produce request.
## 'none': Fire-and-forget, no ack is sent from Kafka.
# bridge.kafka.producer.required_acks = none

## default=10000. Timeout for the Kafka partition leader to wait for its replicas
## to ack before sending back the ack to the producer.
## When this timeout happens on Kafka side, its typically an indicator of Kafka
## being overloaded, an error code is sent back to EMQX's Kafka producer,
## but the producer will retry indefinitely.
# bridge.kafka.producer.ack_timeout = 10S

## Size limit for the Kafka producer to collect small messages to make a
## message batch towards Kafka.
## Kafka has a default value of 1MB message size limit, here we default
## to 900KB is to preserve some bytes for the message encoding overheads
# bridge.kafka.producer.max_batch_bytes = 900KB

## Number of batches to be sent ahead without receiving ack for the last request.
## Having it sent to greater than zero may help to gain throughput, but the risk
## is messages may delivered out of order due to fail + retry.
## NOTE: Must be 0 if messages must be delivered in strict order.
# bridge.kafka.producer.max_send_ahead = 0

## by default, no compression
# bridge.kafka.producer.compression = no_compression

## Specify the interval for refreshing the number of Kafka topic partitions
## used for Kafka topic partition expansion
## by default, 60s

#bridge.kafka.producer.partition_count_refresh_interval_seconds = 60s

## MQTT message payload is embedded as a field of a structured message towards Kafka.
## This config can either be 'base64' or 'plain'
##
## By default the MQTT payload is base64 encoded, and embedded as a 'payload' field
## of a JSON envelop message.
##
## When configured to 'plain', the payload is encoded as a 'byte' field of an
## avro-binary envelop. The avro schema files can be found in the install directory
## of emqx_bridge_kafka-<app_vsn>/priv
# bridge.kafka.encode_payload_type = base64

## Below configs are to tune socket send or receive buffers.
# bridge.kafka.sock.buffer = 32KB
# bridge.kafka.sock.recbuf = 32KB
bridge.kafka.sock.sndbuf = 1MB
# bridge.kafka.sock.read_packets = 20

## Bridge Kafka Hooks
## ${topic}: the kafka topics to which the messages will be published.
## ${filter}: the mqtt topic (may contain wildcard) on which the action will be performed .
## It's possible to disable the default hooks with an empty value environment variable
## e.g. EMQX_BRIDGE__KAFKA__HOOK__CLIENT__CONNECTED__1=""
## NOTE: Hook points should not share Kafka topics when replayq is enabled.
bridge.kafka.hook.client.connected.1     = {"topic":"ClientConnected"}
bridge.kafka.hook.client.disconnected.1  = {"topic":"ClientDisconnected"}
bridge.kafka.hook.session.subscribed.1   = {"filter":"#", "topic":"SessionSubscribed"}
bridge.kafka.hook.session.unsubscribed.1 = {"filter":"#", "topic":"SessionUnsubscribed"}
bridge.kafka.hook.message.publish.1      = {"filter":"#", "topic":"MessagePublish"}
bridge.kafka.hook.message.delivered.1    = {"filter":"#", "topic":"MessageDelivered"}
bridge.kafka.hook.message.acked.1        = {"filter":"#", "topic":"MessageAcked"}

## More Configures
## partitioner strategy:
## Option:  random | roundrobin | key_dispatch
## Example: bridge.kafka.hook.message.publish.1 = {"filter":"#", "topic":"MessagePublish", "strategy":"random"}

## key:
## Option: ${clientid} | ${username}
## Example: bridge.kafka.hook.message.publish.1 = {"filter":"#", "topic":"MessagePublish", "key":"${clientid}"}

## format:
## Option: json | json
## Example: bridge.kafka.hook.message.publish.1 = {"filter":"#", "topic":"MessagePublish", "format":"json"}
